{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50ad7233-8685-4e61-876f-efc3a57d3c43",
   "metadata": {},
   "source": [
    "# üìä YouTube Trend Analytics ‚Äî Script Generation Notebook\n",
    "\n",
    "This notebook prepares all essential Python scripts required for the **Daily YouTube Trend Automation System**, including:\n",
    "\n",
    "- Fetching supported country codes\n",
    "- Fetching YouTube category mappings\n",
    "- Utility function for converting video durations\n",
    "- Core script for fetching trending data for any region\n",
    "- Main automation script used in GitHub Actions\n",
    "\n",
    "All files generated from this notebook will be committed to the GitHub repository and used by the daily automation workflow.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d05e6d-cf8a-44bd-970d-6ed140c837c3",
   "metadata": {},
   "source": [
    "## üåç Fetch All Supported YouTube Countries\n",
    "\n",
    "YouTube supports trending videos for many regions.  \n",
    "This cell:\n",
    "- Loads the YouTube Data API key from the environment\n",
    "- Calls the `i18nRegions` endpoint\n",
    "- Extracts the list of region codes (IN, US, GB, BR, etc.)\n",
    "- Saves them into `countries.json`\n",
    "\n",
    "These region codes are used later for fetching trending videos for each country.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "586a64ea-e448-4cd6-a2e1-0c20dd7038a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Load environment variables (YOUTUBE_API_KEY)\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize YouTube Data API client\n",
    "API_KEY = os.getenv('YOUTUBE_API_KEY')\n",
    "YOUTUBE = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "def get_all_countries():\n",
    "    \"\"\"\n",
    "    Fetch all supported YouTube region codes (e.g., IN, US, BR, GB...).\n",
    "    Returns a list of country codes.\n",
    "    \"\"\"\n",
    "    request = YOUTUBE.i18nRegions().list(part=\"snippet\")\n",
    "    response = request.execute()\n",
    "\n",
    "    countries = []\n",
    "    for item in response[\"items\"]:\n",
    "        countries.append(item[\"id\"])\n",
    "    return countries\n",
    "\n",
    "# Fetch countries and save to JSON file\n",
    "countries = get_all_countries()\n",
    "with open(\"../countries.json\", \"w\") as f:\n",
    "    json.dump(countries, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0319e82d-0699-4d41-a046-a9baa4d8ee54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AE', 'BH', 'DZ', 'EG', 'IQ']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print first 5 elements of the countries list\n",
    "countries[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9534822d-c5b8-4c44-bafb-3c31a993c271",
   "metadata": {},
   "source": [
    "## üóÇÔ∏è Generate Category Mapping File\n",
    "\n",
    "This cell:\n",
    "- Fetches video categories for each country\n",
    "- Merges each one into a single dictionary\n",
    "- Saves the category ID ‚Üí category name mapping into `categories.json`\n",
    "\n",
    "This file is used by all future scripts to translate YouTube's category IDs to readable names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcf5873f-c61c-4868-90d6-71f88e11b889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category_map():\n",
    "    \"\"\"\n",
    "    Fetch YouTube video categories for each country, merge them and return a mapping:\n",
    "        {category_id: category_name}\n",
    "    \"\"\"\n",
    "    categories = {} \n",
    "    for country in countries:\n",
    "        request = YOUTUBE.videoCategories().list(\n",
    "            part=\"snippet\",\n",
    "            regionCode=country\n",
    "        )\n",
    "        response = request.execute()\n",
    "        \n",
    "        for item in response[\"items\"]:\n",
    "            if item[\"id\"] not in categories:\n",
    "                categories[item[\"id\"]] = item[\"snippet\"][\"title\"]\n",
    "    return categories\n",
    "\n",
    "# Fetch categories and save to JSON file\n",
    "categories = get_category_map()\n",
    "with open(\"../categories.json\", \"w\") as f:\n",
    "    json.dump(categories, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36789132-95b6-4968-92a4-86eb87188a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': 'Film & Animation',\n",
       " '2': 'Autos & Vehicles',\n",
       " '10': 'Music',\n",
       " '15': 'Pets & Animals',\n",
       " '17': 'Sports',\n",
       " '18': 'Short Movies',\n",
       " '19': 'Travel & Events',\n",
       " '20': 'Gaming',\n",
       " '21': 'Videoblogging',\n",
       " '22': 'People & Blogs',\n",
       " '23': 'Comedy',\n",
       " '24': 'Entertainment',\n",
       " '25': 'News & Politics',\n",
       " '26': 'Howto & Style',\n",
       " '27': 'Education',\n",
       " '28': 'Science & Technology',\n",
       " '30': 'Movies',\n",
       " '31': 'Anime/Animation',\n",
       " '32': 'Action/Adventure',\n",
       " '33': 'Classics',\n",
       " '34': 'Comedy',\n",
       " '35': 'Documentary',\n",
       " '36': 'Drama',\n",
       " '37': 'Family',\n",
       " '38': 'Foreign',\n",
       " '39': 'Horror',\n",
       " '40': 'Sci-Fi/Fantasy',\n",
       " '41': 'Thriller',\n",
       " '42': 'Shorts',\n",
       " '43': 'Shows',\n",
       " '44': 'Trailers',\n",
       " '29': 'Nonprofits & Activism'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print categories dictionary\n",
    "categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138df15c-ff76-4c5d-8a42-ed56ab2a5444",
   "metadata": {},
   "source": [
    "## ‚è±Ô∏è Create Duration Conversion Utility\n",
    "\n",
    "This script (`convert_duration.py`) contains a helper function that converts\n",
    "ISO-8601 duration strings such as:\n",
    "\n",
    "- PT5M20S  \n",
    "- PT1H2M5S  \n",
    "- PT30S  \n",
    "\n",
    "into total seconds.\n",
    "\n",
    "This utility will be imported inside the trending fetch script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "65a1352c-342e-4bd4-b96b-1875e4265fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../scripts/convert_duration.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../scripts/convert_duration.py\n",
    "import re\n",
    "\n",
    "def duration_to_seconds(duration):\n",
    "    \"\"\"\n",
    "    Convert ISO-8601 YouTube duration strings into total seconds.\n",
    "\n",
    "    Examples:\n",
    "        PT5M20S ‚Üí 320 seconds\n",
    "        PT1H2M5S ‚Üí 3725 seconds\n",
    "        PT30S ‚Üí 30 seconds\n",
    "\n",
    "    Parameters:\n",
    "        duration (str): ISO-8601 duration string\n",
    "\n",
    "    Returns:\n",
    "        int: Duration in seconds\n",
    "    \"\"\"\n",
    "    match = re.match(r'PT(?:(\\d+)H)?(?:(\\d+)M)?(?:(\\d+)S)?', duration)\n",
    "    if not match:\n",
    "        return 0\n",
    "    \n",
    "    hours = int(match.group(1)) if match.group(1) else 0\n",
    "    minutes = int(match.group(2)) if match.group(2) else 0\n",
    "    seconds = int(match.group(3)) if match.group(3) else 0\n",
    "    \n",
    "    return hours * 3600 + minutes * 60 + seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6935b5-c42c-4070-8f16-b2ca5883a0a2",
   "metadata": {},
   "source": [
    "## üì• Create Script to Fetch Trending Videos\n",
    "\n",
    "This script (`fetch_youtube_trend.py`) defines the function `get_trending_videos(region)`:\n",
    "\n",
    "- Calls the YouTube API for a specific region\n",
    "- Retrieves snippet, statistics, and content details\n",
    "- Maps video category IDs ‚Üí category names using `categories.json`\n",
    "- Converts video duration into seconds\n",
    "- Returns a clean pandas DataFrame\n",
    "\n",
    "This script is used by the main automation workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6117a014-91b6-44f9-ab74-4032be10e558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../scripts/fetch_youtube_trend.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../scripts/fetch_youtube_trend.py\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime, timezone\n",
    "from convert_duration import duration_to_seconds\n",
    "\n",
    "# Load API Key\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY = os.getenv('YOUTUBE_API_KEY')\n",
    "YOUTUBE = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "# Load category mapping\n",
    "with open(\"../categories.json\", \"r\") as f:\n",
    "    cat_map = json.load(f)\n",
    "\n",
    "def get_trending_videos(region):\n",
    "    \"\"\"\n",
    "    Fetch top 50 trending videos for a given region.\n",
    "\n",
    "    Parameters:\n",
    "        region (str): Country code (e.g., 'IN', 'US')\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned table of trending videos and metadata\n",
    "    \"\"\"\n",
    "    request = YOUTUBE.videos().list(\n",
    "        part=\"snippet,statistics,contentDetails\",\n",
    "        chart=\"mostPopular\",\n",
    "        regionCode=region,\n",
    "        maxResults=50\n",
    "    )\n",
    "    response = request.execute()\n",
    "\n",
    "    rows = []\n",
    "    for v in response.get(\"items\", []):\n",
    "        snippet = v[\"snippet\"]\n",
    "        stats = v.get(\"statistics\", {})\n",
    "        content = v.get(\"contentDetails\", {})\n",
    "        status = v.get(\"status\", {})\n",
    "        cid = snippet.get(\"categoryId\")\n",
    "\n",
    "        rows.append({\n",
    "            \"video_id\": v[\"id\"],\n",
    "            \"country\": region,\n",
    "            \"fetched_at\": datetime.now(timezone.utc).isoformat(),\n",
    "\n",
    "            # Snippet\n",
    "            \"published_at\": snippet.get(\"publishedAt\"),\n",
    "            \"title\": snippet.get(\"title\"),\n",
    "            \"localized_title\": snippet.get(\"localized\", {}).get(\"title\"),\n",
    "            \"channel_title\": snippet.get(\"channelTitle\"),\n",
    "            \"channel_id\": snippet.get(\"channelId\"),\n",
    "            \"category_id\": cid,\n",
    "            \"category_name\": cat_map.get(cid, \"Unknown\"),\n",
    "            \"tags\": \", \".join(snippet.get(\"tags\", [])),\n",
    "            \"tag_count\": len(snippet.get(\"tags\", [])),\n",
    "            \"thumbnail\": snippet.get(\"thumbnails\", {}).get(\"high\", {}).get(\"url\"),\n",
    "            \"default_language\": snippet.get(\"defaultLanguage\"),\n",
    "            \"audio_language\": snippet.get(\"defaultAudioLanguage\"),\n",
    "            \"is_live\": snippet.get(\"liveBroadcastContent\") == \"live\",\n",
    "\n",
    "            # Content details\n",
    "            \"duration\": duration_to_seconds(content.get(\"duration\")),\n",
    "            \"duration_raw\": content.get(\"duration\"),\n",
    "            \"definition\": content.get(\"definition\"),\n",
    "            \"caption_available\": content.get(\"caption\") == \"true\",\n",
    "            \"licensed_content\": content.get(\"licensedContent\", False),\n",
    "            \"embeddable\": status.get(\"embeddable\", False),\n",
    "            \"made_for_kids\": status.get(\"madeForKids\", False),\n",
    "\n",
    "            # Stats\n",
    "            \"views\": int(stats.get(\"viewCount\", 0)),\n",
    "            \"likes\": int(stats.get(\"likeCount\", 0)),\n",
    "            \"comments\": int(stats.get(\"commentCount\", 0)),\n",
    "        })\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1dab3a-6231-440d-8cb8-3ae2da31652a",
   "metadata": {},
   "source": [
    "## üì• Create Script to Fetch Trending Channels\n",
    "\n",
    "This script (`fetch_youtube_channels.py`) defines the function `get_trending_channels(channel_ids)`:\n",
    "\n",
    "* Calls the YouTube API for one or more channels\n",
    "* Retrieves snippet, statistics, branding, status, and topic details\n",
    "* Cleans topic category URLs into readable topics\n",
    "* Returns a pandas DataFrame with detailed channel metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69f466cd-f28d-476f-830e-6979fbcf2f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../scripts/fetch_youtube_channels.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../scripts/fetch_youtube_channels.py\n",
    "from googleapiclient.discovery import build\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY = os.getenv(\"YOUTUBE_API_KEY\")\n",
    "YOUTUBE = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "\n",
    "def get_trending_channels(channel_ids):\n",
    "    \"\"\"\n",
    "    Fetch channel details for one or multiple channels.\n",
    "\n",
    "    Parameters:\n",
    "        channel_ids (str or list): single channel_id or list of channel_ids (max 50 per request)\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: channel info including snippet, statistics, branding, status\n",
    "    \"\"\"\n",
    "    request = YOUTUBE.channels().list(\n",
    "        part=\"snippet,statistics,brandingSettings,status,topicDetails\",\n",
    "        id=\",\".join(channel_ids)\n",
    "    )\n",
    "    response = request.execute()\n",
    "\n",
    "    rows = []\n",
    "    for ch in response.get(\"items\", []):\n",
    "        snippet = ch.get(\"snippet\", {})\n",
    "        stats = ch.get(\"statistics\", {})\n",
    "        branding = ch.get(\"brandingSettings\", {}).get(\"image\", {})\n",
    "        status = ch.get(\"status\", {})\n",
    "\n",
    "        topic_details = ch.get(\"topicDetails\", {})\n",
    "        topic_ids = topic_details.get(\"topicIds\", [])\n",
    "        topic_categories = topic_details.get(\"topicCategories\", []) # Wikipedia URLs\n",
    "        cleaned_topics = [t.split('/')[-1] for t in topic_categories]\n",
    "\n",
    "        rows.append({\n",
    "            # Basic IDs\n",
    "            \"channel_id\": ch.get(\"id\"),\n",
    "            \"title\": snippet.get(\"title\"),\n",
    "            \"description\": snippet.get(\"description\"),\n",
    "            \"published_at\": snippet.get(\"publishedAt\"),\n",
    "            \"thumbnails\": snippet.get(\"thumbnails\", {}).get(\"high\", {}).get(\"url\"),\n",
    "            \"custom_url\": snippet.get(\"customUrl\"),\n",
    "            \"default_language\": snippet.get(\"defaultLanguage\"),\n",
    "            \"country\": snippet.get(\"country\"),\n",
    "\n",
    "            # Statistics\n",
    "            \"subscriber_count\": int(stats.get(\"subscriberCount\", 0)),\n",
    "            \"video_count\": int(stats.get(\"videoCount\", 0)),\n",
    "            \"view_count\": int(stats.get(\"viewCount\", 0)),\n",
    "\n",
    "            # Branding\n",
    "            \"banner_url\": branding.get(\"bannerExternalUrl\"),\n",
    "            \"keywords\": ch.get(\"brandingSettings\", {}).get(\"channel\", {}).get(\"keywords\"),\n",
    "            \"topics\": \", \".join(cleaned_topics),\n",
    "\n",
    "            # Status\n",
    "            \"made_for_kids\": status.get(\"madeForKids\", False),\n",
    "            \"privacy_status\": status.get(\"privacyStatus\"),\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818b8aea-5283-4898-a295-92cf9d2871fc",
   "metadata": {},
   "source": [
    "## üöÄ Main Automation Workflow ‚Äì Fetch Trending Videos & Channels\n",
    "\n",
    "This script (`main.py`) orchestrates the full data collection pipeline for YouTube trending analytics.\n",
    "\n",
    "### üìå What this script does\n",
    "\n",
    "The `run()` function performs the following steps:\n",
    "\n",
    "* Loads the list of YouTube-supported countries from `countries.json`\n",
    "* Iterates through each country and:\n",
    "\n",
    "  * Fetches top 50 trending videos using `get_trending_videos()`\n",
    "  * Saves country-level daily CSV files in a partitioned directory structure\n",
    "    (`country=XX/year=YYYY/month=MM`)\n",
    "  * Collects **unique channel IDs** from trending videos\n",
    "* Compares newly discovered channel IDs with existing stored channels\n",
    "* Fetches **only new channel metadata** using `get_trending_channels()`\n",
    "* Appends new channel data to a master `trending_channels.csv` file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05a57e06-7fb0-4249-8ca6-6330039cd84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../scripts/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../scripts/main.py\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Import existing functions\n",
    "from fetch_youtube_trend import get_trending_videos\n",
    "from fetch_youtube_channels import get_trending_channels\n",
    "\n",
    "# Load list of YouTube-supported countries\n",
    "with open(\"../countries.json\", 'r') as f:\n",
    "    countries = json.load(f)\n",
    "\n",
    "def run():\n",
    "    \"\"\"\n",
    "    Main workflow:\n",
    "    - Create folder structure for the current year\n",
    "    - Fetch trending videos for every country\n",
    "    - Save country-level CSV files\n",
    "    - Update Master Channel List\n",
    "    \"\"\"\n",
    "    today = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "    year = today.split(\"-\")[0]\n",
    "    month = today.split(\"-\")[1]\n",
    "    \n",
    "    # Robust path handling\n",
    "    BASE_DIR = os.path.dirname(os.path.abspath('.'))\n",
    "    DATA_DIR = os.path.join(BASE_DIR, \"..\", \"data\")\n",
    "\n",
    "    # To store unique channel ids found TODAY\n",
    "    today_channel_ids = np.array([], dtype=str)\n",
    "\n",
    "    # --- STEP 1: FETCH VIDEOS ---\n",
    "    for country in countries:\n",
    "        print(f\"\\nFetching trending videos for {country}...\")\n",
    "\n",
    "        try:\n",
    "            df = get_trending_videos(country)\n",
    "\n",
    "            # Skip empty results\n",
    "            if df.empty:\n",
    "                print(f\"No trending data for {country}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Folder for the specific country and the date\n",
    "            COUNTRY_DIR = os.path.join(DATA_DIR, \"videos\", f\"country={country}\", f\"year={year}\", f\"month={month}\")\n",
    "            os.makedirs(COUNTRY_DIR, exist_ok=True)\n",
    "\n",
    "            # Save daily file\n",
    "            file_path = os.path.join(COUNTRY_DIR, f\"trending_{country}_{today}.csv\")\n",
    "            df.to_csv(file_path, index=False)\n",
    "            print(f\"Saved ‚Üí {file_path}\")\n",
    "\n",
    "            # Collect unique channels from this batch\n",
    "            if 'channel_id' in df.columns:\n",
    "                unique_channels_current = df['channel_id'].dropna().astype(str).unique()\n",
    "                today_channel_ids = np.union1d(today_channel_ids, unique_channels_current)\n",
    "\n",
    "            time.sleep(0.5)  # Avoid API quota bursts\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {country}: {e}\")\n",
    "\n",
    "    # --- STEP 2: UPDATE MASTER CHANNEL LIST ---\n",
    "    channels_dir = os.path.join(DATA_DIR, \"channels\")\n",
    "    os.makedirs(channels_dir, exist_ok=True)\n",
    "    channels_path = os.path.join(channels_dir, \"trending_channels.csv\")\n",
    "\n",
    "    # Handle First Run (File doesn't exist yet)\n",
    "    if os.path.exists(channels_path):\n",
    "        channels_df = pd.read_csv(channels_path)\n",
    "        existing_ids = channels_df['channel_id'].astype(str).unique()\n",
    "    else:\n",
    "        print(\"\\nMaster channel file not found. Creating new one.\")\n",
    "        channels_df = pd.DataFrame(columns=['channel_id'])\n",
    "        existing_ids = np.array([], dtype=str)\n",
    "\n",
    "    # Calculate which channels are actually NEW\n",
    "    # np.setdiff1d returns elements in 'today_channel_ids' that are NOT in 'existing_ids'\n",
    "    new_channels_to_fetch = np.setdiff1d(today_channel_ids, existing_ids)\n",
    "\n",
    "    print(f\"\\nTotal channels found today: {len(today_channel_ids)}\")\n",
    "    print(f\"New channels to fetch: {len(new_channels_to_fetch)}\")\n",
    "\n",
    "    if len(new_channels_to_fetch) == 0:\n",
    "        print(\"No new channels to fetch. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Batch fetch new channels\n",
    "    new_channel_dfs = []\n",
    "    \n",
    "    # Convert numpy array to python list for safety\n",
    "    fetch_list = new_channels_to_fetch.tolist()\n",
    "\n",
    "    for i in range(0, len(fetch_list), 50):\n",
    "        batch = fetch_list[i:i+50]\n",
    "        try:\n",
    "            print(f\"Fetching channel batch {i} to {i+len(batch)}...\")\n",
    "            temp_df = get_trending_channels(batch)\n",
    "            \n",
    "            if not temp_df.empty:\n",
    "                new_channel_dfs.append(temp_df)\n",
    "            \n",
    "            time.sleep(0.5) # Be nice to the API\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Channel fetch failed for batch {i}: {e}\")\n",
    "\n",
    "    # Save Updates\n",
    "    if new_channel_dfs:\n",
    "        new_data_df = pd.concat(new_channel_dfs, ignore_index=True)\n",
    "        \n",
    "        # Combine old data + new data\n",
    "        updated_channels_df = pd.concat([channels_df, new_data_df], ignore_index=True)\n",
    "        \n",
    "        # Save back to CSV\n",
    "        updated_channels_df.to_csv(channels_path, index=False)\n",
    "        print(f\"‚úÖ Updated master channel list. Total channels: {len(updated_channels_df)}\")\n",
    "    else:\n",
    "        print(\"‚ùå Warning: New channels were identified but fetch returned no data.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
